{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LibraryCode.MLP import MLP,DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../Session 3/datasets/'\n",
    "dir_root = '../Session 3/savedParams/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_data, \"words_idfs.txt\")) as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "def loadDataset():\n",
    "    test_data_reader = DataReader(\n",
    "        data_path=os.path.join(dir_data, \"data_tf_idf.txt\"),\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "        size=(0.8, 1)\n",
    "    )\n",
    "    train_data_reader = DataReader(\n",
    "        data_path=os.path.join(dir_data, \"data_tf_idf.txt\"),\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveParameters(name, value, epoch):\n",
    "    filename = name.replace(\":\", '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = \",\".join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([\",\".join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "    with open(os.path.join(dir_root, filename), \"w\") as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreParameter(name, epoch):\n",
    "    filename = name.replace(\":\", '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open(os.path.join(dir_root, filename)) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(\",\")]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(\",\")] for row in range(len(lines))]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(vocab_size=vocab_size, hidden_size=50)\n",
    "pred_y, loss = mlp.buildGraph()\n",
    "train_op = mlp.trainer(loss=loss, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, loss: 0.0004734218819066882\n",
      "Step: 1000, loss: 0.027874385938048363\n",
      "Step: 1500, loss: 2.3245736429089447e-06\n",
      "Step: 2000, loss: 4.329438888817094e-05\n",
      "Step: 2500, loss: 8.396965313295368e-06\n",
      "Step: 3000, loss: 0.00013136008055880666\n",
      "Step: 3500, loss: 0.00011709719547070563\n",
      "Step: 4000, loss: 0.09098311513662338\n",
      "Step: 4500, loss: 1.3925422430038452\n",
      "Step: 5000, loss: 3.542870899764239e-06\n",
      "Step: 5500, loss: 1.4280637515184935e-05\n",
      "Step: 6000, loss: 0.0\n",
      "Step: 6500, loss: 0.0\n",
      "Step: 7000, loss: 0.0\n",
      "Step: 7500, loss: 0.0\n",
      "Step: 8000, loss: 0.0\n",
      "Step: 8500, loss: 0.0\n",
      "Step: 9000, loss: 0.0\n",
      "Step: 9500, loss: 0.0\n",
      "Step: 10000, loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    train_data_reader, test_data_reader = loadDataset()\n",
    "    step, MAX_STEP = 0, 10000\n",
    "    \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.nextBatch()\n",
    "        plabels_eval, loss_eval, _ = sess.run(\n",
    "            [pred_y, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        step += 1\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step: {step}, loss: {loss_eval}\")\n",
    "        \n",
    "    trainable_variables = tf.compat.v1.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saveParameters(\n",
    "            name=variable.name,\n",
    "            value=variable.eval(),\n",
    "            epoch=train_data_reader._num_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on the batch 500\n",
      "Test on the batch 1000\n",
      "Test on the batch 1500\n",
      "Test on the batch 2000\n",
      "Test on the batch 2500\n",
      "Test on the batch 3000\n",
      "========\n",
      "Epoch: 0\n",
      "Accuracy on the test data: 0.008222811671087533\n"
     ]
    }
   ],
   "source": [
    "# Open a session to test the model\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    _, test_data_reader = loadDataset()\n",
    "    step, MAX_STEP = 0, 3000\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        epoch = 0\n",
    "        trainable_variables = tf.compat.v1.trainable_variables()\n",
    "        for variable in trainable_variables:\n",
    "            saved_value = restoreParameter(\n",
    "                name=variable.name,\n",
    "                epoch=epoch\n",
    "            )\n",
    "            assign_op = variable.assign(saved_value)\n",
    "            sess.run(assign_op)\n",
    "        \n",
    "        num_true_preds = 0\n",
    "        while step < MAX_STEP:\n",
    "            test_data, test_labels = test_data_reader.nextBatch()\n",
    "            test_plabels_eval = sess.run(\n",
    "                pred_y,\n",
    "                feed_dict={\n",
    "                    mlp._X: test_data,\n",
    "                    mlp._real_Y: test_labels\n",
    "                }\n",
    "            )\n",
    "            matches = np.equal(test_plabels_eval, test_labels)\n",
    "            num_true_preds += np.sum(matches.astype(\"float\"))\n",
    "            \n",
    "            step += 1\n",
    "            if step % 500 == 0:\n",
    "                print(f\"Test on the batch {step}\")\n",
    "\n",
    "            if test_data_reader._batch_id == 0:\n",
    "                break\n",
    "        \n",
    "        print(\"========\")\n",
    "        print(\"Epoch:\", epoch)\n",
    "        print(\"Accuracy on the test data:\", num_true_preds / len(test_data_reader._data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5c16fd675006009e5e0850495db5a6d9a0a8ddbda09f4f68b3c97a5bba8d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
