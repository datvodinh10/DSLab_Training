{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Member**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member:\n",
    "    def __init__(self,r_d,label=None,doc_id=None):\n",
    "        self._r_d = r_d\n",
    "        self._label = label\n",
    "        self._doc_id = doc_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self) -> None:\n",
    "        self._centroid = None\n",
    "        self._members = []\n",
    "    \n",
    "    def resetMembers(self):\n",
    "        self._members = []\n",
    "    \n",
    "    def addMembers(self,member):\n",
    "        self._members.append(member)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "class Kmeans():\n",
    "    def __init__(self,num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = [Cluster() for _ in range(self._num_clusters)]\n",
    "        self._E = [] # list of centroid\n",
    "        self._S = 0 # overall simularity\n",
    "        self._centroids = []\n",
    "    def loadData(self,path):\n",
    "        def sparseToDense(sparse_r_d, vocab_size):\n",
    "            r_d = [0.0 for _ in range(vocab_size)]\n",
    "            indices_tfidf = sparse_r_d.split()\n",
    "            for index_tfidf in indices_tfidf:\n",
    "                index = int(index_tfidf.split(':')[0])\n",
    "                tfidf = float(index_tfidf.split(':')[1])\n",
    "                r_d[index] = tfidf\n",
    "            return np.array(r_d)\n",
    "\n",
    "        with open(os.path.join(path, \"data_tf_idf.txt\")) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        with open(os.path.join(path, \"words_idfs.txt\")) as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "\n",
    "        self._data = []\n",
    "        self._label_count = defaultdict(int)\n",
    "        for data_id, d in enumerate(d_lines):\n",
    "            features = d.split('<fff>')\n",
    "            label, doc_id = int(features[0]),int(features[1])\n",
    "            self._label_count[label]+=1\n",
    "            r_d = sparseToDense(sparse_r_d=features[2],vocab_size=vocab_size)\n",
    "            self._data.append(Member(r_d,label,doc_id))\n",
    "    \n",
    "    def randomInit(self,seed_value):\n",
    "        random.seed(seed_value)\n",
    "        data_shuffled = random.choices(self._data, k=self._num_clusters)\n",
    "        for i in range(self._num_clusters):\n",
    "            centroid = data_shuffled[i]._r_d\n",
    "            self._clusters[i]._centroid = centroid\n",
    "            self._centroids.append(centroid)\n",
    "    \n",
    "    def computeSimilarity(self,member,centroid):\n",
    "        return sum(member._r_d * centroid) / np.sqrt(np.sum(member._r_d ** 2) * np.sum(centroid ** 2))\n",
    "    \n",
    "    def selectClusterFor(self,member):\n",
    "        best_fit_cluster = None\n",
    "        max_similarity = -1\n",
    "        for cluster in self._clusters:\n",
    "            similarity = self.computeSimilarity(member, cluster._centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = similarity\n",
    "        # add data point to cluster._members (list)\n",
    "        best_fit_cluster.addMembers(member)\n",
    "        return max_similarity\n",
    "\n",
    "    def updateCentroidOf(self,cluster):\n",
    "        average_rd = np.mean([member._r_d for member in cluster._members], axis=0)\n",
    "        sum_squares = np.sum(average_rd ** 2)\n",
    "        # normalize average rd\n",
    "        new_centroid = np.array([num / np.sqrt(sum_squares) for num in average_rd])\n",
    "\n",
    "        cluster._centroid = new_centroid\n",
    "    \n",
    "    def stoppingCondition(self,criterion,threshold):\n",
    "        criteria = ['centroid', 'similarity', 'max_iters']\n",
    "        assert criterion in criteria\n",
    "\n",
    "        if criterion == 'max_iters':\n",
    "            if self._iteration  >= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif criterion == 'centroid':\n",
    "            # after calculating centroids, before re-positioning centroid\n",
    "            centroids_new = [list(cluster._centroid) for cluster in self._clusters]\n",
    "            centroids_changes = [centroid for centroid in centroids_new if centroid not in self._centroids]\n",
    "            self._centroids = centroids_new\n",
    "            \n",
    "            if len(centroids_changes) <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        elif criterion == 'similarity':\n",
    "            # while running algorithm, keep separated previous clustering error (S) and new clustering error (new_S)\n",
    "            clustering_error_change = self._new_S - self._S\n",
    "            self._S = self._new_S\n",
    "            if clustering_error_change <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def run(self,seed_value,criterion,threshold):\n",
    "        self.randomInit(seed_value)\n",
    "        self._iteration = 0\n",
    "        while True:\n",
    "            for cluster in self._clusters:\n",
    "                cluster.resetMembers()\n",
    "            self._new_S = 0\n",
    "            for member in self._data:\n",
    "                max_s = self.selectClusterFor(member)\n",
    "                self._new_S+=max_s\n",
    "            for cluster in self._clusters:\n",
    "                self.updateCentroidOf(cluster)\n",
    "\n",
    "            self._iteration+=1\n",
    "            if self.stoppingCondition(criterion,threshold):\n",
    "                break\n",
    "\n",
    "    \n",
    "    def computePurity(self):\n",
    "        majority_sum = 0\n",
    "        for cluster in self._clusters:\n",
    "            member_labels = [member._label for member in cluster._members ]\n",
    "            max_count = max([member_labels.count(label) for label in range(20)])\n",
    "            majority_sum+=max_count\n",
    "            return majority_sum * 1. / len(self._data)\n",
    "    \n",
    "    def computeNMI(self):\n",
    "        I_value, H_omega, H_C, N = 0., 0., 0., len(self._data)\n",
    "        for cluster in self._clusters:\n",
    "            wk = len(cluster._members) * 1. \n",
    "            H_omega += - wk / N * np.log10(wk / N)\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "        for label in range(20):\n",
    "            wk_cj = member_labels.count(label) * 1.\n",
    "            cj = self._label_count[label]\n",
    "            I_value += wk_cj / N * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
    "        for label in range(20):\n",
    "            cj = self._label_count[label] * 1.\n",
    "            H_C += - cj / N * np.log10(cj / N)\n",
    "        return I_value * 2. / (H_omega + H_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = Kmeans(num_clusters=8)\n",
    "kmeans.loadData('../Session1/Data/20news-bydate.tar/20news-bydate/')\n",
    "kmeans.run(seed_value=42, criterion='similarity', threshold=1e-3)\n",
    "print(f\"Purity: {kmeans.computePurity()}\", f\"\\nNMI: {kmeans.computeNMI()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(path):\n",
    "    def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "        r_d = [0.0 for _ in range(vocab_size)]\n",
    "        indices_and_tfidfs = sparse_r_d.split()\n",
    "        for index_and_tfidf in indices_and_tfidfs:\n",
    "            index = int(index_and_tfidf.split(':')[0])\n",
    "            tfidf = float(index_and_tfidf.split(':')[1])\n",
    "            r_d[index] = tfidf\n",
    "        return np.array(r_d)    \n",
    "                \n",
    "    with open(os.path.join(path, \"data_tf_idf.txt\")) as f:\n",
    "            data_lines = f.read().splitlines()\n",
    "    with open(os.path.join(path, \"words_idfs.txt\")) as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "\n",
    "    data, labels = [], []\n",
    "    for data_id, d in enumerate(data_lines):\n",
    "        features = d.split('<fff>')\n",
    "        label, doc_id = int(features[0]), int(features[1])\n",
    "        r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
    "        data.append(r_d)\n",
    "        labels.append(label)\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# extract the data\n",
    "X, y = load_data('../Session1/Data/20news-bydate.tar/20news-bydate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    # Check boolean True = 1, False = 0 \n",
    "    matches = np.equal(y_pred, y)\n",
    "    accuracy = np.sum(matches.astype(float)) / len(y)\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sklearn Kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringWithKMeans():\n",
    "    data,labels = load_data('../Session1/Data/20news-bydate.tar/20news-bydate/')\n",
    "    from sklearn import cluster\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    X = csr_matrix(data)\n",
    "    print(\"========\")\n",
    "    kmeans = cluster.KMeans(\n",
    "        n_clusters=8, \n",
    "        init=\"random\",\n",
    "        n_init=10,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "        ).fit(X)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clusteringWithKMeans()\n",
    "compute_accuracy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM\n",
    "def classifyingWithLinearSVM(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    clf = LinearSVC(C=10.0, tol=0.001, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = compute_accuracy(y_pred, y_test)\n",
    "    print(\"\\nAccuracy:\", accuracy)\n",
    "    return clf\n",
    "\n",
    "classifyingWithLinearSVM(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel SVM\n",
    "def classifyingWithKernelSVM(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC(C=50.0, kernel=\"rbf\", gamma=0.1, tol=0.001, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = compute_accuracy(y_pred, y_test)\n",
    "    print(\"\\nAccuracy:\", accuracy)\n",
    "    return clf\n",
    "\n",
    "classifyingWithKernelSVM(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5581cd8f8e7f555d1c7b7d5c73b743c62e9c35962a29bf47b3ccdfb22fa58433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
